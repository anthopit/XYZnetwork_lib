{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-22T21:48:17.531679Z",
     "end_time": "2023-04-22T21:48:35.945464Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessing as pp\n",
    "from classes import transportnetwork as tn\n",
    "from data import *\n",
    "from model import *\n",
    "from run import *\n",
    "from visualisation.visualisation import *\n",
    "from characterisation.page_rank import *\n",
    "from networkx import NetworkXNoPath\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69638/69638 [00:16<00:00, 4238.07it/s]\n"
     ]
    }
   ],
   "source": [
    "G = pp.create_network_from_trailway(\"../data/Railway Data_JL.xlsx\")\n",
    "TN = tn.TransportNetwork(G, pos_argument=['lon', 'lat'], time_arguments=['dep_time', 'arr_time'], distance_argument='distance')\n",
    "\n",
    "graph=TN.get_higher_complexity()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T21:48:35.922353Z",
     "end_time": "2023-04-22T21:51:06.214848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"node_features\" : [\"degree_one_hot\"], # choices are [\"degree_one_hot\", \"one_hot\", \"constant\", \"pagerank\", \"degree\", \"betweenness\", \"closeness\", \"eigenvector\", \"clustering\", \"position\", \"distance\"]\n",
    "    \"node_attrs\" : None,\n",
    "    \"edge_attrs\" : None, # choices are [\"distance\", \"dep_time\", \"arr_time\"]\n",
    "    \"train_ratio\" : 0.8,\n",
    "    \"val_ratio\" : 0.1,\n",
    "\n",
    "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"model\" : \"gat\", # choices are [\"gcn\", \"gin\", \"gat\", \"sage\"]\n",
    "    \"layers\" : 2,\n",
    "    \"hidden_channels\" : 128,\n",
    "    \"dim_embedding\" : 64,\n",
    "    \"save\" : \"ssl_model.pth\",\n",
    "\n",
    "    \"lr\" : 0.001,\n",
    "    \"epochs\" : 200,\n",
    "    \"num_workers\" : 4,\n",
    "\n",
    "    \"loss\" : \"infonce\",\n",
    "    \"augment_list\" : [\"edge_perturbation\", \"node_dropping\"],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T21:51:06.206495Z",
     "end_time": "2023-04-22T21:51:06.221269Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1512.,  692., 2335.,  ..., 2682., 2657., 2684.])\n",
      "Data(edge_index=[2, 42082], lon=[2175], lat=[2175], dep_time=[42082], arr_time=[42082], train=[64155], train_max_speed=[64155], day=[64155], distance=[42082], euclidian_distance=[42082], num_nodes=2175, x=[2175, 481], train_mask=[2175], val_mask=[2175], y=[2175])\n",
      "2175\n",
      "Data(edge_index=[2, 784], lon=[271], lat=[271], dep_time=[784], arr_time=[784], train=[64155], train_max_speed=[64155], day=[64155], distance=[784], euclidian_distance=[784], num_nodes=271, x=[271, 481], train_mask=[271], val_mask=[271], y=[271])\n",
      "271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([2175])) that is different to the input size (torch.Size([2175, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([271])) that is different to the input size (torch.Size([271, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1220.7713, Validation Loss: 9252.6541\n",
      "Epoch: 2, Train Loss: 1220.6299, Validation Loss: 9251.4437\n",
      "Epoch: 3, Train Loss: 1220.4936, Validation Loss: 9250.1863\n",
      "Epoch: 4, Train Loss: 1220.3433, Validation Loss: 9248.8293\n",
      "Epoch: 5, Train Loss: 1220.1801, Validation Loss: 9247.3293\n",
      "Epoch: 6, Train Loss: 1219.9959, Validation Loss: 9245.6356\n",
      "Epoch: 7, Train Loss: 1219.8030, Validation Loss: 9243.8081\n",
      "Epoch: 8, Train Loss: 1219.5852, Validation Loss: 9241.8367\n",
      "Epoch: 9, Train Loss: 1219.3493, Validation Loss: 9239.7168\n",
      "Epoch: 10, Train Loss: 1219.1044, Validation Loss: 9237.4419\n",
      "Epoch: 11, Train Loss: 1218.8230, Validation Loss: 9235.0065\n",
      "Epoch: 12, Train Loss: 1218.5266, Validation Loss: 9232.4077\n",
      "Epoch: 13, Train Loss: 1218.2448, Validation Loss: 9229.6402\n",
      "Epoch: 14, Train Loss: 1217.9239, Validation Loss: 9226.6993\n",
      "Epoch: 15, Train Loss: 1217.5711, Validation Loss: 9223.5793\n",
      "Epoch: 16, Train Loss: 1217.1910, Validation Loss: 9220.2758\n",
      "Epoch: 17, Train Loss: 1216.8011, Validation Loss: 9216.7869\n",
      "Epoch: 18, Train Loss: 1216.4149, Validation Loss: 9213.1052\n",
      "Epoch: 19, Train Loss: 1215.9441, Validation Loss: 9209.2279\n",
      "Epoch: 20, Train Loss: 1215.5207, Validation Loss: 9205.1494\n",
      "Epoch: 21, Train Loss: 1215.0749, Validation Loss: 9200.8672\n",
      "Epoch: 22, Train Loss: 1214.5320, Validation Loss: 9196.3736\n",
      "Epoch: 23, Train Loss: 1214.0179, Validation Loss: 9191.6651\n",
      "Epoch: 24, Train Loss: 1213.4043, Validation Loss: 9186.7371\n",
      "Epoch: 25, Train Loss: 1212.9143, Validation Loss: 9181.5849\n",
      "Epoch: 26, Train Loss: 1212.2969, Validation Loss: 9176.2076\n",
      "Epoch: 27, Train Loss: 1211.7003, Validation Loss: 9170.5950\n",
      "Epoch: 28, Train Loss: 1211.0279, Validation Loss: 9164.7463\n",
      "Epoch: 29, Train Loss: 1210.2700, Validation Loss: 9158.6513\n",
      "Epoch: 30, Train Loss: 1209.5498, Validation Loss: 9152.3081\n",
      "Epoch: 31, Train Loss: 1208.8661, Validation Loss: 9145.7085\n",
      "Epoch: 32, Train Loss: 1208.1636, Validation Loss: 9138.8552\n",
      "Epoch: 33, Train Loss: 1207.3082, Validation Loss: 9131.7399\n",
      "Epoch: 34, Train Loss: 1206.4144, Validation Loss: 9124.3598\n",
      "Epoch: 35, Train Loss: 1205.5314, Validation Loss: 9116.7103\n",
      "Epoch: 36, Train Loss: 1204.6084, Validation Loss: 9108.7878\n",
      "Epoch: 37, Train Loss: 1203.7201, Validation Loss: 9100.5932\n",
      "Epoch: 38, Train Loss: 1202.8008, Validation Loss: 9092.1227\n",
      "Epoch: 39, Train Loss: 1201.7131, Validation Loss: 9083.3690\n",
      "Epoch: 40, Train Loss: 1200.9232, Validation Loss: 9074.3413\n",
      "Epoch: 41, Train Loss: 1199.6414, Validation Loss: 9065.0240\n",
      "Epoch: 42, Train Loss: 1198.5808, Validation Loss: 9055.4197\n",
      "Epoch: 43, Train Loss: 1197.4457, Validation Loss: 9045.5240\n",
      "Epoch: 44, Train Loss: 1196.2949, Validation Loss: 9035.3376\n",
      "Epoch: 45, Train Loss: 1195.1293, Validation Loss: 9024.8561\n",
      "Epoch: 46, Train Loss: 1193.8398, Validation Loss: 9014.0803\n",
      "Epoch: 47, Train Loss: 1192.7897, Validation Loss: 9003.0111\n",
      "Epoch: 48, Train Loss: 1191.4160, Validation Loss: 8991.6448\n",
      "Epoch: 49, Train Loss: 1190.2839, Validation Loss: 8979.9852\n",
      "Epoch: 50, Train Loss: 1188.7024, Validation Loss: 8968.0249\n",
      "Epoch: 51, Train Loss: 1187.3507, Validation Loss: 8955.7611\n",
      "Epoch: 52, Train Loss: 1185.7462, Validation Loss: 8943.1928\n",
      "Epoch: 53, Train Loss: 1184.2889, Validation Loss: 8930.3183\n",
      "Epoch: 54, Train Loss: 1182.8122, Validation Loss: 8917.1384\n",
      "Epoch: 55, Train Loss: 1181.4855, Validation Loss: 8903.6531\n",
      "Epoch: 56, Train Loss: 1179.7172, Validation Loss: 8889.8635\n",
      "Epoch: 57, Train Loss: 1177.8291, Validation Loss: 8875.7574\n",
      "Epoch: 58, Train Loss: 1176.2268, Validation Loss: 8861.3413\n",
      "Epoch: 59, Train Loss: 1174.5369, Validation Loss: 8846.6135\n",
      "Epoch: 60, Train Loss: 1173.1139, Validation Loss: 8831.5793\n",
      "Epoch: 61, Train Loss: 1171.2333, Validation Loss: 8816.2371\n",
      "Epoch: 62, Train Loss: 1169.7176, Validation Loss: 8800.5913\n",
      "Epoch: 63, Train Loss: 1167.6267, Validation Loss: 8784.6402\n",
      "Epoch: 64, Train Loss: 1165.6926, Validation Loss: 8768.3792\n",
      "Epoch: 65, Train Loss: 1163.9055, Validation Loss: 8751.8118\n",
      "Epoch: 66, Train Loss: 1161.9175, Validation Loss: 8734.9391\n",
      "Epoch: 67, Train Loss: 1160.4756, Validation Loss: 8717.7712\n",
      "Epoch: 68, Train Loss: 1158.1789, Validation Loss: 8700.3026\n",
      "Epoch: 69, Train Loss: 1155.9528, Validation Loss: 8682.5286\n",
      "Epoch: 70, Train Loss: 1153.9914, Validation Loss: 8664.4520\n",
      "Epoch: 71, Train Loss: 1151.6820, Validation Loss: 8646.0710\n",
      "Epoch: 72, Train Loss: 1149.4107, Validation Loss: 8627.3828\n",
      "Epoch: 73, Train Loss: 1147.1408, Validation Loss: 8608.3884\n",
      "Epoch: 74, Train Loss: 1144.2946, Validation Loss: 8589.0784\n",
      "Epoch: 75, Train Loss: 1142.7632, Validation Loss: 8569.4705\n",
      "Epoch: 76, Train Loss: 1140.3421, Validation Loss: 8549.5627\n",
      "Epoch: 77, Train Loss: 1137.8730, Validation Loss: 8529.3579\n",
      "Epoch: 78, Train Loss: 1135.4507, Validation Loss: 8508.8524\n",
      "Epoch: 79, Train Loss: 1133.0026, Validation Loss: 8488.0498\n",
      "Epoch: 80, Train Loss: 1130.7670, Validation Loss: 8466.9557\n",
      "Epoch: 81, Train Loss: 1128.3310, Validation Loss: 8445.5756\n",
      "Epoch: 82, Train Loss: 1126.1351, Validation Loss: 8423.9151\n",
      "Epoch: 83, Train Loss: 1123.4714, Validation Loss: 8401.9696\n",
      "Epoch: 84, Train Loss: 1120.2733, Validation Loss: 8379.7343\n",
      "Epoch: 85, Train Loss: 1118.2559, Validation Loss: 8357.2205\n",
      "Epoch: 86, Train Loss: 1115.4990, Validation Loss: 8334.4290\n",
      "Epoch: 87, Train Loss: 1112.7898, Validation Loss: 8311.3589\n",
      "Epoch: 88, Train Loss: 1110.4089, Validation Loss: 8288.0203\n",
      "Epoch: 89, Train Loss: 1107.6516, Validation Loss: 8264.4133\n",
      "Epoch: 90, Train Loss: 1104.6134, Validation Loss: 8240.5397\n",
      "Epoch: 91, Train Loss: 1102.8682, Validation Loss: 8216.4133\n",
      "Epoch: 92, Train Loss: 1099.3690, Validation Loss: 8192.0277\n",
      "Epoch: 93, Train Loss: 1095.8064, Validation Loss: 8167.3736\n",
      "Epoch: 94, Train Loss: 1092.8208, Validation Loss: 8142.4548\n",
      "Epoch: 95, Train Loss: 1089.9690, Validation Loss: 8117.2712\n",
      "Epoch: 96, Train Loss: 1086.3447, Validation Loss: 8091.8201\n",
      "Epoch: 97, Train Loss: 1084.5248, Validation Loss: 8066.1208\n",
      "Epoch: 98, Train Loss: 1080.8651, Validation Loss: 8040.1688\n",
      "Epoch: 99, Train Loss: 1078.0093, Validation Loss: 8013.9677\n",
      "Epoch: 100, Train Loss: 1075.0116, Validation Loss: 7987.5231\n",
      "Epoch: 101, Train Loss: 1072.3834, Validation Loss: 7960.8459\n",
      "Epoch: 102, Train Loss: 1068.5599, Validation Loss: 7933.9299\n",
      "Epoch: 103, Train Loss: 1065.4797, Validation Loss: 7906.7768\n",
      "Epoch: 104, Train Loss: 1062.7768, Validation Loss: 7879.3976\n",
      "Epoch: 105, Train Loss: 1059.1885, Validation Loss: 7851.7934\n",
      "Epoch: 106, Train Loss: 1056.5606, Validation Loss: 7823.9714\n",
      "Epoch: 107, Train Loss: 1053.8222, Validation Loss: 7795.9410\n",
      "Epoch: 108, Train Loss: 1048.7040, Validation Loss: 7767.6817\n",
      "Epoch: 109, Train Loss: 1045.6576, Validation Loss: 7739.2039\n",
      "Epoch: 110, Train Loss: 1042.4755, Validation Loss: 7710.5138\n",
      "Epoch: 111, Train Loss: 1040.6238, Validation Loss: 7681.6324\n",
      "Epoch: 112, Train Loss: 1035.2161, Validation Loss: 7652.5378\n",
      "Epoch: 113, Train Loss: 1032.7378, Validation Loss: 7623.2458\n",
      "Epoch: 114, Train Loss: 1029.5415, Validation Loss: 7593.7634\n",
      "Epoch: 115, Train Loss: 1026.4264, Validation Loss: 7564.0964\n",
      "Epoch: 116, Train Loss: 1021.6759, Validation Loss: 7534.2352\n",
      "Epoch: 117, Train Loss: 1018.3163, Validation Loss: 7504.1850\n",
      "Epoch: 118, Train Loss: 1015.5599, Validation Loss: 7473.9580\n",
      "Epoch: 119, Train Loss: 1010.9792, Validation Loss: 7443.5484\n",
      "Epoch: 120, Train Loss: 1008.2854, Validation Loss: 7412.9705\n",
      "Epoch: 121, Train Loss: 1003.5352, Validation Loss: 7382.2149\n",
      "Epoch: 122, Train Loss: 1000.9584, Validation Loss: 7351.2975\n",
      "Epoch: 123, Train Loss: 995.9947, Validation Loss: 7320.2025\n",
      "Epoch: 124, Train Loss: 993.0622, Validation Loss: 7288.9460\n",
      "Epoch: 125, Train Loss: 989.3145, Validation Loss: 7257.5337\n",
      "Epoch: 126, Train Loss: 986.5048, Validation Loss: 7225.9792\n",
      "Epoch: 127, Train Loss: 982.1009, Validation Loss: 7194.2763\n",
      "Epoch: 128, Train Loss: 977.5779, Validation Loss: 7162.4220\n",
      "Epoch: 129, Train Loss: 974.2715, Validation Loss: 7130.4271\n",
      "Epoch: 130, Train Loss: 970.7694, Validation Loss: 7098.3012\n",
      "Epoch: 131, Train Loss: 967.4669, Validation Loss: 7066.0512\n",
      "Epoch: 132, Train Loss: 963.5820, Validation Loss: 7033.6827\n",
      "Epoch: 133, Train Loss: 957.8205, Validation Loss: 7001.1794\n",
      "Epoch: 134, Train Loss: 954.3709, Validation Loss: 6968.5507\n",
      "Epoch: 135, Train Loss: 951.1571, Validation Loss: 6935.8077\n",
      "Epoch: 136, Train Loss: 945.8778, Validation Loss: 6902.9405\n",
      "Epoch: 137, Train Loss: 945.1297, Validation Loss: 6869.9839\n",
      "Epoch: 138, Train Loss: 938.8971, Validation Loss: 6836.9197\n",
      "Epoch: 139, Train Loss: 936.1908, Validation Loss: 6803.7643\n",
      "Epoch: 140, Train Loss: 932.2999, Validation Loss: 6770.5212\n",
      "Epoch: 141, Train Loss: 930.8356, Validation Loss: 6737.2149\n",
      "Epoch: 142, Train Loss: 923.5375, Validation Loss: 6703.8206\n",
      "Epoch: 143, Train Loss: 919.2821, Validation Loss: 6670.3404\n",
      "Epoch: 144, Train Loss: 916.3998, Validation Loss: 6636.7823\n",
      "Epoch: 145, Train Loss: 911.5453, Validation Loss: 6603.1518\n",
      "Epoch: 146, Train Loss: 907.5377, Validation Loss: 6569.4511\n",
      "Epoch: 147, Train Loss: 904.7099, Validation Loss: 6535.6970\n",
      "Epoch: 148, Train Loss: 899.3719, Validation Loss: 6501.8773\n",
      "Epoch: 149, Train Loss: 895.0680, Validation Loss: 6467.9991\n",
      "Epoch: 150, Train Loss: 891.9313, Validation Loss: 6434.0747\n",
      "Epoch: 151, Train Loss: 888.2935, Validation Loss: 6400.1125\n",
      "Epoch: 152, Train Loss: 881.8566, Validation Loss: 6366.0936\n",
      "Epoch: 153, Train Loss: 880.3017, Validation Loss: 6332.0475\n",
      "Epoch: 154, Train Loss: 877.2471, Validation Loss: 6297.9825\n",
      "Epoch: 155, Train Loss: 872.6413, Validation Loss: 6263.8981\n",
      "Epoch: 156, Train Loss: 867.4020, Validation Loss: 6229.7911\n",
      "Epoch: 157, Train Loss: 863.1320, Validation Loss: 6195.6619\n",
      "Epoch: 158, Train Loss: 861.0947, Validation Loss: 6161.5304\n",
      "Epoch: 159, Train Loss: 854.4541, Validation Loss: 6127.3815\n",
      "Epoch: 160, Train Loss: 853.2782, Validation Loss: 6093.2417\n",
      "Epoch: 161, Train Loss: 845.2424, Validation Loss: 6059.0890\n",
      "Epoch: 162, Train Loss: 843.1266, Validation Loss: 6024.9391\n",
      "Epoch: 163, Train Loss: 839.2071, Validation Loss: 5990.7980\n",
      "Epoch: 164, Train Loss: 834.0262, Validation Loss: 5956.6628\n",
      "Epoch: 165, Train Loss: 830.0549, Validation Loss: 5922.5415\n",
      "Epoch: 166, Train Loss: 826.3518, Validation Loss: 5888.4437\n",
      "Epoch: 167, Train Loss: 824.7514, Validation Loss: 5854.3824\n",
      "Epoch: 168, Train Loss: 819.8321, Validation Loss: 5820.3565\n",
      "Epoch: 169, Train Loss: 815.6770, Validation Loss: 5786.3695\n",
      "Epoch: 170, Train Loss: 810.2107, Validation Loss: 5752.4193\n",
      "Epoch: 171, Train Loss: 805.3710, Validation Loss: 5718.5078\n",
      "Epoch: 172, Train Loss: 800.0069, Validation Loss: 5684.6338\n",
      "Epoch: 173, Train Loss: 800.2916, Validation Loss: 5650.8224\n",
      "Epoch: 174, Train Loss: 796.0666, Validation Loss: 5617.0756\n",
      "Epoch: 175, Train Loss: 790.8916, Validation Loss: 5583.3893\n",
      "Epoch: 176, Train Loss: 789.0421, Validation Loss: 5549.7809\n",
      "Epoch: 177, Train Loss: 782.3570, Validation Loss: 5516.2394\n",
      "Epoch: 178, Train Loss: 778.4341, Validation Loss: 5482.7726\n",
      "Epoch: 179, Train Loss: 775.5360, Validation Loss: 5449.3888\n",
      "Epoch: 180, Train Loss: 772.2406, Validation Loss: 5416.0927\n",
      "Epoch: 181, Train Loss: 767.4191, Validation Loss: 5382.8833\n",
      "Epoch: 182, Train Loss: 759.3814, Validation Loss: 5349.7495\n",
      "Epoch: 183, Train Loss: 760.1190, Validation Loss: 5316.7113\n",
      "Epoch: 184, Train Loss: 754.7925, Validation Loss: 5283.7694\n",
      "Epoch: 185, Train Loss: 748.5303, Validation Loss: 5250.9170\n",
      "Epoch: 186, Train Loss: 744.3818, Validation Loss: 5218.1582\n",
      "Epoch: 187, Train Loss: 740.4026, Validation Loss: 5185.5018\n",
      "Epoch: 188, Train Loss: 738.9564, Validation Loss: 5152.9617\n",
      "Epoch: 189, Train Loss: 733.8289, Validation Loss: 5120.5360\n",
      "Epoch: 190, Train Loss: 727.9056, Validation Loss: 5088.2251\n",
      "Epoch: 191, Train Loss: 726.7363, Validation Loss: 5056.0360\n",
      "Epoch: 192, Train Loss: 722.8811, Validation Loss: 5023.9779\n",
      "Epoch: 193, Train Loss: 718.7357, Validation Loss: 4992.0489\n",
      "Epoch: 194, Train Loss: 717.9840, Validation Loss: 4960.2620\n",
      "Epoch: 195, Train Loss: 711.8885, Validation Loss: 4928.6167\n",
      "Epoch: 196, Train Loss: 708.2059, Validation Loss: 4897.1149\n",
      "Epoch: 197, Train Loss: 704.2185, Validation Loss: 4865.7615\n",
      "Epoch: 198, Train Loss: 698.8316, Validation Loss: 4834.5535\n",
      "Epoch: 199, Train Loss: 696.2203, Validation Loss: 4803.5000\n",
      "Epoch: 200, Train Loss: 689.4051, Validation Loss: 4772.5978\n",
      "Epoch: 201, Train Loss: 688.9997, Validation Loss: 4741.8556\n",
      "Epoch: 202, Train Loss: 688.6103, Validation Loss: 4711.2869\n",
      "Epoch: 203, Train Loss: 679.6743, Validation Loss: 4680.8801\n",
      "Epoch: 204, Train Loss: 674.3718, Validation Loss: 4650.6365\n",
      "Epoch: 205, Train Loss: 675.6099, Validation Loss: 4620.5655\n",
      "Epoch: 206, Train Loss: 674.4757, Validation Loss: 4590.6776\n",
      "Epoch: 207, Train Loss: 665.4508, Validation Loss: 4560.9673\n",
      "Epoch: 208, Train Loss: 665.6221, Validation Loss: 4531.4423\n",
      "Epoch: 209, Train Loss: 658.9428, Validation Loss: 4502.0996\n",
      "Epoch: 210, Train Loss: 655.5174, Validation Loss: 4472.9396\n",
      "Epoch: 211, Train Loss: 649.6754, Validation Loss: 4443.9617\n",
      "Epoch: 212, Train Loss: 649.8589, Validation Loss: 4415.1771\n",
      "Epoch: 213, Train Loss: 645.5052, Validation Loss: 4386.5835\n",
      "Epoch: 214, Train Loss: 639.9449, Validation Loss: 4358.1854\n",
      "Epoch: 215, Train Loss: 637.3014, Validation Loss: 4329.9871\n",
      "Epoch: 216, Train Loss: 636.1051, Validation Loss: 4301.9903\n",
      "Epoch: 217, Train Loss: 628.5882, Validation Loss: 4274.1983\n",
      "Epoch: 218, Train Loss: 626.7024, Validation Loss: 4246.6112\n",
      "Epoch: 219, Train Loss: 627.2756, Validation Loss: 4219.2339\n",
      "Epoch: 220, Train Loss: 621.8923, Validation Loss: 4192.0701\n",
      "Epoch: 221, Train Loss: 615.9024, Validation Loss: 4165.1149\n",
      "Epoch: 222, Train Loss: 612.1903, Validation Loss: 4138.3727\n",
      "Epoch: 223, Train Loss: 609.7643, Validation Loss: 4111.8487\n",
      "Epoch: 224, Train Loss: 610.1149, Validation Loss: 4085.5470\n",
      "Epoch: 225, Train Loss: 605.5586, Validation Loss: 4059.4682\n",
      "Epoch: 226, Train Loss: 600.5306, Validation Loss: 4033.6112\n",
      "Epoch: 227, Train Loss: 599.8882, Validation Loss: 4007.9806\n",
      "Epoch: 228, Train Loss: 595.7267, Validation Loss: 3982.5784\n",
      "Epoch: 229, Train Loss: 588.9864, Validation Loss: 3957.4082\n",
      "Epoch: 230, Train Loss: 587.8788, Validation Loss: 3932.4668\n",
      "Epoch: 231, Train Loss: 589.4280, Validation Loss: 3907.7555\n",
      "Epoch: 232, Train Loss: 583.0249, Validation Loss: 3883.2804\n",
      "Epoch: 233, Train Loss: 577.7219, Validation Loss: 3859.0422\n",
      "Epoch: 234, Train Loss: 575.0268, Validation Loss: 3835.0443\n",
      "Epoch: 235, Train Loss: 571.0036, Validation Loss: 3811.2892\n",
      "Epoch: 236, Train Loss: 572.4606, Validation Loss: 3787.7696\n",
      "Epoch: 237, Train Loss: 565.6641, Validation Loss: 3764.4933\n",
      "Epoch: 238, Train Loss: 560.4214, Validation Loss: 3741.4585\n",
      "Epoch: 239, Train Loss: 563.3484, Validation Loss: 3718.6617\n",
      "Epoch: 240, Train Loss: 559.9782, Validation Loss: 3696.1070\n",
      "Epoch: 241, Train Loss: 555.3007, Validation Loss: 3673.8024\n",
      "Epoch: 242, Train Loss: 551.3898, Validation Loss: 3651.7452\n",
      "Epoch: 243, Train Loss: 550.6348, Validation Loss: 3629.9306\n",
      "Epoch: 244, Train Loss: 548.2702, Validation Loss: 3608.3545\n",
      "Epoch: 245, Train Loss: 544.5065, Validation Loss: 3587.0247\n",
      "Epoch: 246, Train Loss: 540.1967, Validation Loss: 3565.9377\n",
      "Epoch: 247, Train Loss: 539.1045, Validation Loss: 3545.0985\n",
      "Epoch: 248, Train Loss: 535.3122, Validation Loss: 3524.5111\n",
      "Epoch: 249, Train Loss: 533.9948, Validation Loss: 3504.1737\n",
      "Epoch: 250, Train Loss: 530.4694, Validation Loss: 3484.0872\n",
      "Epoch: 251, Train Loss: 527.4395, Validation Loss: 3464.2537\n",
      "Epoch: 252, Train Loss: 525.6512, Validation Loss: 3444.6656\n",
      "Epoch: 253, Train Loss: 525.5028, Validation Loss: 3425.3280\n",
      "Epoch: 254, Train Loss: 521.1661, Validation Loss: 3406.2405\n",
      "Epoch: 255, Train Loss: 519.1417, Validation Loss: 3387.4038\n",
      "Epoch: 256, Train Loss: 514.0746, Validation Loss: 3368.8220\n",
      "Epoch: 257, Train Loss: 512.1889, Validation Loss: 3350.4954\n",
      "Epoch: 258, Train Loss: 513.6652, Validation Loss: 3332.4124\n",
      "Epoch: 259, Train Loss: 510.0148, Validation Loss: 3314.5763\n",
      "Epoch: 260, Train Loss: 508.7861, Validation Loss: 3296.9845\n",
      "Epoch: 261, Train Loss: 501.5501, Validation Loss: 3279.6511\n",
      "Epoch: 262, Train Loss: 502.3137, Validation Loss: 3262.5613\n",
      "Epoch: 263, Train Loss: 498.1560, Validation Loss: 3245.7239\n",
      "Epoch: 264, Train Loss: 495.9679, Validation Loss: 3229.1430\n",
      "Epoch: 265, Train Loss: 493.0117, Validation Loss: 3212.8125\n",
      "Epoch: 266, Train Loss: 490.7871, Validation Loss: 3196.7378\n",
      "Epoch: 267, Train Loss: 489.3077, Validation Loss: 3180.9154\n",
      "Epoch: 268, Train Loss: 488.9822, Validation Loss: 3165.3323\n",
      "Epoch: 269, Train Loss: 487.4774, Validation Loss: 3150.0021\n",
      "Epoch: 270, Train Loss: 484.7644, Validation Loss: 3134.9133\n",
      "Epoch: 271, Train Loss: 484.9424, Validation Loss: 3120.0544\n",
      "Epoch: 272, Train Loss: 480.9503, Validation Loss: 3105.4405\n",
      "Epoch: 273, Train Loss: 479.6434, Validation Loss: 3091.0669\n",
      "Epoch: 274, Train Loss: 474.6747, Validation Loss: 3076.9357\n",
      "Epoch: 275, Train Loss: 476.9308, Validation Loss: 3063.0362\n",
      "Epoch: 276, Train Loss: 473.0661, Validation Loss: 3049.3715\n",
      "Epoch: 277, Train Loss: 472.2943, Validation Loss: 3035.9430\n",
      "Epoch: 278, Train Loss: 468.5006, Validation Loss: 3022.7498\n",
      "Epoch: 279, Train Loss: 466.1093, Validation Loss: 3009.8035\n",
      "Epoch: 280, Train Loss: 466.1375, Validation Loss: 2997.0899\n",
      "Epoch: 281, Train Loss: 462.2079, Validation Loss: 2984.6137\n",
      "Epoch: 282, Train Loss: 461.8680, Validation Loss: 2972.3639\n",
      "Epoch: 283, Train Loss: 461.2386, Validation Loss: 2960.3321\n",
      "Epoch: 284, Train Loss: 457.9219, Validation Loss: 2948.5238\n",
      "Epoch: 285, Train Loss: 457.1173, Validation Loss: 2936.9366\n",
      "Epoch: 286, Train Loss: 455.9099, Validation Loss: 2925.5729\n",
      "Epoch: 287, Train Loss: 454.0269, Validation Loss: 2914.4246\n",
      "Epoch: 288, Train Loss: 453.9283, Validation Loss: 2903.4926\n",
      "Epoch: 289, Train Loss: 456.0857, Validation Loss: 2892.7567\n",
      "Epoch: 290, Train Loss: 449.1739, Validation Loss: 2882.2461\n",
      "Epoch: 291, Train Loss: 451.0080, Validation Loss: 2871.9363\n",
      "Epoch: 292, Train Loss: 448.1703, Validation Loss: 2861.8268\n",
      "Epoch: 293, Train Loss: 448.2021, Validation Loss: 2851.9147\n",
      "Epoch: 294, Train Loss: 441.8976, Validation Loss: 2842.2219\n",
      "Epoch: 295, Train Loss: 445.5346, Validation Loss: 2832.7313\n",
      "Epoch: 296, Train Loss: 443.3504, Validation Loss: 2823.4407\n",
      "Epoch: 297, Train Loss: 441.6583, Validation Loss: 2814.3443\n",
      "Epoch: 298, Train Loss: 442.0565, Validation Loss: 2805.4449\n",
      "Epoch: 299, Train Loss: 437.7746, Validation Loss: 2796.7484\n",
      "Epoch: 300, Train Loss: 437.6678, Validation Loss: 2788.2403\n",
      "Epoch: 301, Train Loss: 434.5998, Validation Loss: 2779.9308\n",
      "Epoch: 302, Train Loss: 433.6792, Validation Loss: 2771.8180\n",
      "Epoch: 303, Train Loss: 432.4824, Validation Loss: 2763.8988\n",
      "Epoch: 304, Train Loss: 431.9639, Validation Loss: 2756.1605\n",
      "Epoch: 305, Train Loss: 432.2955, Validation Loss: 2748.5999\n",
      "Epoch: 306, Train Loss: 431.5064, Validation Loss: 2741.2149\n",
      "Epoch: 307, Train Loss: 429.8154, Validation Loss: 2734.0067\n",
      "Epoch: 308, Train Loss: 427.9502, Validation Loss: 2726.9749\n",
      "Epoch: 309, Train Loss: 426.2004, Validation Loss: 2720.1179\n",
      "Epoch: 310, Train Loss: 425.1710, Validation Loss: 2713.4366\n",
      "Epoch: 311, Train Loss: 427.3535, Validation Loss: 2706.9147\n",
      "Epoch: 312, Train Loss: 424.8848, Validation Loss: 2700.5494\n",
      "Epoch: 313, Train Loss: 424.0493, Validation Loss: 2694.3443\n",
      "Epoch: 314, Train Loss: 421.6929, Validation Loss: 2688.2998\n",
      "Epoch: 315, Train Loss: 421.8339, Validation Loss: 2682.4107\n",
      "Epoch: 316, Train Loss: 419.6469, Validation Loss: 2676.6776\n",
      "Epoch: 317, Train Loss: 418.1620, Validation Loss: 2671.1049\n",
      "Epoch: 318, Train Loss: 417.9584, Validation Loss: 2665.6889\n",
      "Epoch: 319, Train Loss: 416.0661, Validation Loss: 2660.4255\n",
      "Epoch: 320, Train Loss: 417.1650, Validation Loss: 2655.3040\n",
      "Epoch: 321, Train Loss: 417.4640, Validation Loss: 2650.3243\n",
      "Epoch: 322, Train Loss: 414.4697, Validation Loss: 2645.4882\n",
      "Epoch: 323, Train Loss: 413.8572, Validation Loss: 2640.7984\n",
      "Epoch: 324, Train Loss: 412.6523, Validation Loss: 2636.2491\n",
      "Epoch: 325, Train Loss: 413.9052, Validation Loss: 2631.8342\n",
      "Epoch: 326, Train Loss: 412.8618, Validation Loss: 2627.5480\n",
      "Epoch: 327, Train Loss: 410.3769, Validation Loss: 2623.3941\n",
      "Epoch: 328, Train Loss: 412.3435, Validation Loss: 2619.3595\n",
      "Epoch: 329, Train Loss: 409.1359, Validation Loss: 2615.4513\n",
      "Epoch: 330, Train Loss: 408.4083, Validation Loss: 2611.6589\n",
      "Epoch: 331, Train Loss: 409.2121, Validation Loss: 2607.9917\n",
      "Epoch: 332, Train Loss: 410.7722, Validation Loss: 2604.4322\n",
      "Epoch: 333, Train Loss: 406.5485, Validation Loss: 2600.9887\n",
      "Epoch: 334, Train Loss: 405.3852, Validation Loss: 2597.6665\n",
      "Epoch: 335, Train Loss: 405.6276, Validation Loss: 2594.4585\n",
      "Epoch: 336, Train Loss: 404.7261, Validation Loss: 2591.3575\n",
      "Epoch: 337, Train Loss: 404.6740, Validation Loss: 2588.3595\n",
      "Epoch: 338, Train Loss: 403.4624, Validation Loss: 2585.4668\n",
      "Epoch: 339, Train Loss: 404.5733, Validation Loss: 2582.6732\n",
      "Epoch: 340, Train Loss: 402.8555, Validation Loss: 2579.9804\n",
      "Epoch: 341, Train Loss: 402.2358, Validation Loss: 2577.3819\n",
      "Epoch: 342, Train Loss: 401.6421, Validation Loss: 2574.8879\n",
      "Epoch: 343, Train Loss: 403.1847, Validation Loss: 2572.4788\n",
      "Epoch: 344, Train Loss: 404.4495, Validation Loss: 2570.1582\n",
      "Epoch: 345, Train Loss: 402.9025, Validation Loss: 2567.9216\n",
      "Epoch: 346, Train Loss: 401.3860, Validation Loss: 2565.7631\n",
      "Epoch: 347, Train Loss: 399.3635, Validation Loss: 2563.6933\n",
      "Epoch: 348, Train Loss: 399.0345, Validation Loss: 2561.7062\n",
      "Epoch: 349, Train Loss: 399.5711, Validation Loss: 2559.8070\n",
      "Epoch: 350, Train Loss: 398.2063, Validation Loss: 2557.9869\n",
      "Epoch: 351, Train Loss: 401.0785, Validation Loss: 2556.2410\n",
      "Epoch: 352, Train Loss: 398.7546, Validation Loss: 2554.5713\n",
      "Epoch: 353, Train Loss: 398.4130, Validation Loss: 2552.9698\n",
      "Epoch: 354, Train Loss: 396.7100, Validation Loss: 2551.4417\n",
      "Epoch: 355, Train Loss: 397.6866, Validation Loss: 2549.9813\n",
      "Epoch: 356, Train Loss: 396.0971, Validation Loss: 2548.5842\n",
      "Epoch: 357, Train Loss: 397.0513, Validation Loss: 2547.2523\n",
      "Epoch: 358, Train Loss: 398.2930, Validation Loss: 2545.9809\n",
      "Epoch: 359, Train Loss: 395.9779, Validation Loss: 2544.7717\n",
      "Epoch: 360, Train Loss: 395.4110, Validation Loss: 2543.6215\n",
      "Epoch: 361, Train Loss: 395.6595, Validation Loss: 2542.5270\n",
      "Epoch: 362, Train Loss: 395.4662, Validation Loss: 2541.4915\n",
      "Epoch: 363, Train Loss: 392.7195, Validation Loss: 2540.5127\n",
      "Epoch: 364, Train Loss: 393.5096, Validation Loss: 2539.5846\n",
      "Epoch: 365, Train Loss: 393.5661, Validation Loss: 2538.7099\n",
      "Epoch: 366, Train Loss: 394.5916, Validation Loss: 2537.8845\n",
      "Epoch: 367, Train Loss: 393.5729, Validation Loss: 2537.1070\n",
      "Epoch: 368, Train Loss: 394.2370, Validation Loss: 2536.3736\n",
      "Epoch: 369, Train Loss: 391.9689, Validation Loss: 2535.6873\n",
      "Epoch: 370, Train Loss: 393.1579, Validation Loss: 2535.0473\n",
      "Epoch: 371, Train Loss: 393.0944, Validation Loss: 2534.4465\n",
      "Epoch: 372, Train Loss: 392.8558, Validation Loss: 2533.8886\n",
      "Epoch: 373, Train Loss: 393.5844, Validation Loss: 2533.3692\n",
      "Epoch: 374, Train Loss: 391.3931, Validation Loss: 2532.8914\n",
      "Epoch: 375, Train Loss: 391.1827, Validation Loss: 2532.4481\n",
      "Epoch: 376, Train Loss: 391.7638, Validation Loss: 2532.0371\n",
      "Epoch: 377, Train Loss: 392.5037, Validation Loss: 2531.6610\n",
      "Epoch: 378, Train Loss: 391.1992, Validation Loss: 2531.3160\n",
      "Epoch: 379, Train Loss: 391.0033, Validation Loss: 2531.0042\n",
      "Epoch: 380, Train Loss: 391.9524, Validation Loss: 2530.7216\n",
      "Epoch: 381, Train Loss: 391.3879, Validation Loss: 2530.4679\n",
      "Epoch: 382, Train Loss: 392.3704, Validation Loss: 2530.2417\n",
      "Epoch: 383, Train Loss: 391.4256, Validation Loss: 2530.0422\n",
      "Epoch: 384, Train Loss: 390.2640, Validation Loss: 2529.8692\n",
      "Epoch: 385, Train Loss: 389.6950, Validation Loss: 2529.7219\n",
      "Epoch: 386, Train Loss: 389.9003, Validation Loss: 2529.5978\n",
      "Epoch: 387, Train Loss: 392.0310, Validation Loss: 2529.4975\n",
      "Epoch: 388, Train Loss: 389.9447, Validation Loss: 2529.4197\n",
      "Epoch: 389, Train Loss: 390.6318, Validation Loss: 2529.3623\n",
      "Epoch: 390, Train Loss: 389.7612, Validation Loss: 2529.3254\n",
      "Epoch: 391, Train Loss: 389.9286, Validation Loss: 2529.3074\n",
      "Epoch: 392, Train Loss: 389.4335, Validation Loss: 2529.3074\n",
      "Epoch: 393, Train Loss: 390.2817, Validation Loss: 2529.3247\n",
      "Epoch: 394, Train Loss: 391.3001, Validation Loss: 2529.3605\n",
      "Epoch: 395, Train Loss: 391.4641, Validation Loss: 2529.4119\n",
      "Epoch: 396, Train Loss: 388.5978, Validation Loss: 2529.4769\n",
      "Epoch: 397, Train Loss: 390.2303, Validation Loss: 2529.5572\n",
      "Epoch: 398, Train Loss: 389.1153, Validation Loss: 2529.6508\n",
      "Epoch: 399, Train Loss: 389.3506, Validation Loss: 2529.7601\n",
      "Epoch: 400, Train Loss: 388.3607, Validation Loss: 2529.8819\n"
     ]
    }
   ],
   "source": [
    "class AttributeDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttributeDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "args = AttributeDict(args)\n",
    "\n",
    "data = create_data_from_transport_network(graph, TN, node_features=args.node_features, edge_attrs=args.edge_attrs, train_ratio=args.train_ratio, val_ratio=args.val_ratio, num_workers=args.num_workers)\n",
    "\n",
    "# Open a csv in dataframe\n",
    "df = pd.read_csv('../playground/charviz/robustness.csv')\n",
    "\n",
    "# Create a tensor from the dataframe\n",
    "tensor = torch.tensor(df.values).float()\n",
    "# Keep only the first column\n",
    "tensor = tensor[:,0]\n",
    "\n",
    "data.y = tensor\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(data.num_node_features, 64, 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def train(train_data):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    train_data = train_data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion(output[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_all += loss.item()\n",
    "    return loss_all / len(train_data.y)\n",
    "\n",
    "def validate(val_data):\n",
    "    model.eval()\n",
    "    loss_all = 0\n",
    "    val_data = val_data.to(device)\n",
    "    output = model(val_data.x, val_data.edge_index)\n",
    "    loss = criterion(output[val_data.val_mask], val_data.y[val_data.val_mask])\n",
    "    loss_all += loss.item()\n",
    "    return loss_all / len(val_data.y)\n",
    "\n",
    "\n",
    "train_data = data.subgraph(data.train_mask)\n",
    "print(train_data)\n",
    "print(train_data.num_nodes)\n",
    "\n",
    "val_data = data.subgraph(data.val_mask)\n",
    "print(val_data)\n",
    "print(val_data.num_nodes)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(400):\n",
    "    train_loss = train(train_data)\n",
    "    val_loss = validate(val_data)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-22T21:51:06.219327Z",
     "end_time": "2023-04-22T22:29:42.645814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144634/144634 [00:23<00:00, 6220.27it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (613x2171 and 481x64)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 15\u001B[0m\n\u001B[1;32m     11\u001B[0m graph\u001B[38;5;241m=\u001B[39mTN\u001B[38;5;241m.\u001B[39mget_higher_complexity()\n\u001B[1;32m     13\u001B[0m data \u001B[38;5;241m=\u001B[39m create_data_from_transport_network(graph, TN, node_features\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mnode_features, edge_attrs\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39medge_attrs, train_ratio\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mtrain_ratio, val_ratio\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mval_ratio, num_workers\u001B[38;5;241m=\u001B[39margs\u001B[38;5;241m.\u001B[39mnum_workers)\n\u001B[0;32m---> 15\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(new_graph)\u001B[0m\n\u001B[1;32m      3\u001B[0m new_graph \u001B[38;5;241m=\u001B[39m new_graph\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Move the new graph to the device (GPU or CPU)\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():  \u001B[38;5;66;03m# Temporarily disable gradient calculation\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Forward pass through the model\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[4], line 34\u001B[0m, in \u001B[0;36mGNN.forward\u001B[0;34m(self, x, edge_index)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, edge_index):\n\u001B[0;32m---> 34\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     36\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(x, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:229\u001B[0m, in \u001B[0;36mGCNConv.forward\u001B[0;34m(self, x, edge_index, edge_weight)\u001B[0m\n\u001B[1;32m    226\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    227\u001B[0m             edge_index \u001B[38;5;241m=\u001B[39m cache\n\u001B[0;32m--> 229\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001B[39;00m\n\u001B[1;32m    232\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate(edge_index, x\u001B[38;5;241m=\u001B[39mx, edge_weight\u001B[38;5;241m=\u001B[39medge_weight,\n\u001B[1;32m    233\u001B[0m                      size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch_geometric/nn/dense/linear.py:132\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m        x (torch.Tensor): The input features.\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (613x2171 and 481x64)"
     ]
    }
   ],
   "source": [
    "def predict(new_graph):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    new_graph = new_graph.to(device)  # Move the new graph to the device (GPU or CPU)\n",
    "    with torch.no_grad():  # Temporarily disable gradient calculation\n",
    "        output = model(new_graph.x, new_graph.edge_index)  # Forward pass through the model\n",
    "    return output.detach().cpu().numpy()  # Convert the output tensor to a NumPy array\n",
    "\n",
    "G = pp.create_network_from_GTFS(\"../data/gtfs_3\")\n",
    "TN = tn.TransportNetwork(G, pos_argument=['lon', 'lat'], time_arguments=['dep_time', 'arr_time'])\n",
    "\n",
    "graph=TN.get_higher_complexity()\n",
    "\n",
    "data = create_data_from_transport_network(graph, TN, node_features=args.node_features, edge_attrs=args.edge_attrs, train_ratio=args.train_ratio, val_ratio=args.val_ratio, num_workers=args.num_workers)\n",
    "\n",
    "predictions = predict(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
