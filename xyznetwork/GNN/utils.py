import networkx as nx
from torch_geometric.utils import degree, one_hot
import torch.nn.functional as F
import torch
import numpy as np
import multiprocessing as mp
from networkx import NetworkXNoPath
from tqdm import tqdm

class AttributeDict(dict):
    def __init__(self, *args, **kwargs):
        super(AttributeDict, self).__init__(*args, **kwargs)
        self.__dict__ = self



class GNNConfig(AttributeDict):
    """
    Configuration class for Graph Neural Networks (GNNs).
    This class extends the `AttributeDict` class and provides default values for various GNN configuration parameters, such as node features, training settings, and model architecture.
    Attributes
    ----------
    node_features : list of str, optional
        The list of node features to compute. Defaults to ['one_hot'].
    node_attrs : list of str or None, optional
        The list of node attributes to include. Defaults to None.
    edge_attrs : list of str or None, optional
        The list of edge attributes to include. Defaults to None.
    node_label : str or None, optional
        The name of the attribute to use as node label. Defaults to None.
    train_ratio : float, optional
        The ratio of the dataset to be used for training. Defaults to 0.8.
    val_ratio : float, optional
        The ratio of the dataset to be used for validation. Defaults to 0.2.
    device : torch.device, optional
        The device to run the GNN model on. Defaults to GPU if available, else CPU.
    model : str, optional
        The GNN model to use. Defaults to 'gcn'.
    layers : int, optional
        The number of layers in the GNN model. Defaults to 2.
    hidden_dim : int, optional
        The size of the hidden dimension in the GNN model. Defaults to 128.
    dim_embedding : int, optional
        The size of the embeddings generated by the GNN model. Defaults to 64.
    save : str, optional
        The file name to save the trained GNN model. Defaults to 'ssl_model.pth'.
    lr : float, optional
        The learning rate for the GNN model. Defaults to 0.001.
    epochs : int, optional
        The number of epochs to train the GNN model. Defaults to 100.
    num_workers : int, optional
        The number of worker threads to use for parallel computation. Defaults to 1.
    loss : str, optional
        The loss function to use for training the GNN model. Defaults to 'infonce'.
    augment_list : list of str, optional
        The list of data augmentations to apply during training. Defaults to ['edge_perturbation', 'node_dropping'].
    """
    def __init__(self, *args, **kwargs):
        super(GNNConfig, self).__init__(*args, **kwargs)
        self.set_default_values()

    def set_default_values(self):
        self.setdefault('node_features', ['one_hot'])
        self.setdefault('node_attrs', None)
        self.setdefault('edge_attrs', None)
        self.setdefault('node_label', None)
        self.setdefault('train_ratio', 0.8)
        self.setdefault('val_ratio', 0.2)
        self.setdefault('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
        self.setdefault('model', 'gcn')
        self.setdefault('layers', 2)
        self.setdefault('hidden_dim', 128)
        self.setdefault('dim_embedding', 64)
        self.setdefault('save', 'ssl_model.pth')
        self.setdefault('lr', 0.001)
        self.setdefault('epochs', 100)
        self.setdefault('num_workers', 1)
        self.setdefault('loss', 'infonce')
        self.setdefault('augment_list', ['edge_perturbation', 'node_dropping'])

def get_max_deg(data):
    """
    Find the max degree across all nodes in graphs.
    """
    max_deg = 0

    row, col = data.edge_index
    num_nodes = data.num_nodes
    deg = degree(row, num_nodes)
    deg = max(deg).item()
    if deg > max_deg:
        max_deg = int(deg)
    return max_deg

def cat_node_feature(data, graph, TN, cat=True, feature='degree_one_hot', num_workers=1):
    """
    Concatenate node features to the input PyTorch Geometric data object.
    This function takes a PyTorch Geometric data object, a NetworkX graph, and a transport network object, and computes node features based on the specified feature type. The computed features are then concatenated to the input data object's node features (if `cat=True`), or replaces the existing node features (if `cat=False`).
    Parameters
    ----------
    data : PyTorch Geometric Data object
        A graph data object.
    graph : networkx graph
        A NetworkX graph representation of the transport network.
    TN : dict
        A dictionary containing the transport network data.
    cat : bool, optional
        Whether to concatenate the computed features to the existing node features (True) or replace them (False). Defaults to True.
    feature : str, optional
        The type of node feature to compute. Supported values include 'degree_one_hot', 'one_hot', 'constant', 'pagerank', 'degree', 'betweenness', 'closeness', 'eigenvector', 'clustering', 'position', and 'distance'. Defaults to 'degree_one_hot'.
    num_workers : int, optional
        The number of worker threads to use for parallel computation. Defaults to 1.
    Returns
    -------
    data : PyTorch Geometric Data object
        The updated graph data object with the new node features.
    """
    G = graph
    if feature == 'degree_one_hot':
        # Compute the degree of each node
        # and one-hot encode it
        max_degree = get_max_deg(data)
        idx = data.edge_index[0]
        deg = degree(idx, data.num_nodes, dtype=torch.long)
        deg = F.one_hot(deg, num_classes=max_degree + 1).to(torch.float)
        f = deg
    elif feature == 'one_hot':
        indices = torch.tensor([i for i in range(data.num_nodes)])
        one_hot_encoding = one_hot(indices, num_classes=data.num_nodes)
        f = one_hot_encoding
    elif feature == "constant":
        f = torch.ones(data.num_nodes, 1)
    elif feature == "pagerank":
        pagerank = nx.pagerank(G)
        pagerank_features = [pagerank[node] for node in G.nodes()]
        f = torch.tensor(pagerank_features).view(-1, 1).float()
    elif feature == "degree":
        degree_c = nx.degree_centrality(G)
        degree_features = [degree_c[node] for node in G.nodes()]
        f = torch.tensor(degree_features).view(-1, 1).float()
    elif feature == "betweenness":
        centrality = nx.betweenness_centrality(G)
        centrality_features = [centrality[node] for node in G.nodes()]
        f = torch.tensor(centrality_features).view(-1, 1).float()
    elif feature == "closeness":
        centrality = nx.closeness_centrality(G)
        centrality_features = [centrality[node] for node in G.nodes()]
        f = torch.tensor(centrality_features).view(-1, 1).float()
    elif feature == "eigenvector":
        G = TN.graph
        centrality = nx.eigenvector_centrality(G)
        centrality_features = [centrality[node] for node in G.nodes()]
        f = torch.tensor(centrality_features).view(-1, 1).float()
    elif feature == "clustering":
        G = TN.graph
        clustering_coefficient = nx.clustering(G)
        clustering_coefficient_features = [clustering_coefficient[node] for node in G.nodes()]
        f = torch.tensor(clustering_coefficient_features).view(-1, 1).float()
    elif feature == "position":
        anchor_sets = generate_anchor_sets_networkx(G, c=0.5)
        shortest_paths = compute_shortest_paths_parallel(G, anchor_sets, num_workers)
        shortest_paths_list = [shortest_paths[node] for node in G.nodes]
        f = torch.tensor(shortest_paths_list, dtype=torch.float)
    elif feature == "distance":
        if TN.is_spatial:
            anchor_sets = generate_anchor_sets_networkx(G, c=1)
            shortest_paths = compute_shortest_paths_parallel(G, anchor_sets, num_workers, weight='euclidian_distance')
            shortest_paths_list = [shortest_paths[node] for node in G.nodes]
            f = torch.tensor(shortest_paths_list, dtype=torch.float)
        else:
            raise ValueError
    else:
        raise NotImplementedError

    if data.x is not None and cat:
        data.x = torch.cat([data.x, f], dim=-1)
    else:
        data.x = f

    return data


def augment_data(data, augment_list=["edge_perturbation", "node_dropping"], mask_ratio=0.1, mask_mean=0.5, mask_std=0.5, edge_ratio=0.1):
    """
        Augment the input PyTorch Geometric data object with specified data augmentation methods.
        This function takes a PyTorch Geometric data object and applies the specified data augmentation methods, such as edge perturbation and node dropping, with the provided parameters.
        Parameters
        ----------
        data : PyTorch Geometric Data object
            A graph data object.
        augment_list : list of str, optional
            The list of data augmentation methods to apply. Supported values are 'edge_perturbation' and 'node_dropping'. Defaults to ['edge_perturbation', 'node_dropping'].
        mask_ratio : float, optional
            The ratio of nodes to apply the node dropping augmentation. Defaults to 0.1.
        mask_mean : float, optional
            The mean value for generating the random masked node features. Defaults to 0.5.
        mask_std : float, optional
            The standard deviation for generating the random masked node features. Defaults to 0.5.
        edge_ratio : float, optional
            The ratio of edges to perturb in the edge perturbation augmentation. Defaults to 0.1.
        Returns
        -------
        data : PyTorch Geometric Data object
            The augmented graph data object.
        """
    if "edge_perturbation" in augment_list:
        node_num, _ = data.x.size()
        _, edge_num = data.edge_index.size()
        perturb_num = int(edge_num * edge_ratio)

        edge_index = data.edge_index.detach().clone()
        idx_remain = edge_index
        idx_add = torch.tensor([]).reshape(2, -1).long()


        idx_add = torch.randint(node_num, (2, perturb_num))

        new_edge_index = torch.cat((idx_remain, idx_add), dim=1)
        new_edge_index = torch.unique(new_edge_index, dim=1)

        data.edge_index = new_edge_index

    if "node_dropping" in augment_list:
        node_num, feat_dim = data.x.size()
        x = data.x.detach().clone()

        mask = torch.zeros(node_num)
        mask_num = int(node_num * mask_ratio)
        idx_mask = np.random.choice(node_num, mask_num, replace=False)
        x[idx_mask] = torch.tensor(np.random.normal(loc=mask_mean, scale=mask_std,
                                                    size=(mask_num, feat_dim)), dtype=torch.float32)

        data.x = x

    return data

def get_random_anchorset_networkx(graph, c=0.5):
    """
    Generate a list of random anchor sets for a NetworkX graph.
    This function takes a NetworkX graph and generates random anchor sets based on the number of nodes and a specified scaling factor, c. The size of each anchor set is determined based on the number of nodes in the graph and the anchor set level.
    Parameters
    ----------
    graph : networkx.Graph
        The input NetworkX graph.
    c : float, optional
        The scaling factor for the number of anchor sets per level. Defaults to 0.5.
    Returns
    -------
    anchorset_id : list of numpy.ndarray
        A list of random anchor sets, each represented by an array of node indices.
    """
    nodes = list(graph.nodes)
    n = graph.number_of_nodes()
    m = int(np.log2(n))
    copy = int(c * m)
    anchorset_id = []

    copy_num = copy - 1
    for i in range(m):
        anchor_size = int(n / np.exp2(i + 1))

        for j in range(copy - copy_num):
            anchorset_id.append(np.random.choice(nodes, size=anchor_size, replace=False))

        copy_num = copy_num - 1

    return anchorset_id

def generate_anchor_sets_networkx(graph, c=0.5):
    """
    Generates anchor sets for shortest path analyses
    :param graph: Graph to use
    :param c: Parameter c from get_random_anchorset_networkx
    :return: Anchorset list
    """
    # Generate a list of anchor sets
    anchorset_list = get_random_anchorset_networkx(graph, c)

    return anchorset_list

def shortest_paths_worker(args):
    """
    Creates a worker for shortest path analyses
    :param args: Arguments
    """
    G, node, anchor_sets, output_dict, weight = args
    shortest_paths = []
    for anchor_set in anchor_sets:
        anchor_set_shortest_paths = []
        for anchor in anchor_set:
            try:
                if weight is not None:
                    path_length = nx.shortest_path_length(G, source=node, target=anchor, weight=weight)
                else:
                    path_length = nx.shortest_path_length(G, source=node, target=anchor)
            except NetworkXNoPath:
                path_length = 100
            anchor_set_shortest_paths.append(path_length)
        shortest_paths.append(min(anchor_set_shortest_paths))
    output_dict[node] = shortest_paths

def compute_shortest_paths_parallel(G, anchor_sets, num_workers, weight=None):
    """
    Computes shortest path analysis in parallel
    :param G: Graph
    :param anchor_sets: Anchor sets generated by generate_anchor_sets_networkx
    :param num_workers: Number of workers
    :param weight: Which feature for weight on edges/nodes?
    :return: Output dictionary
    """
    nodes = list(G.nodes)
    manager = mp.Manager()
    output_dict = manager.dict()

    with mp.Pool(num_workers) as pool:
        for _ in tqdm(pool.imap_unordered(shortest_paths_worker, [(G, node, anchor_sets, output_dict, weight) for node in nodes]), total=len(nodes)):
            pass

    return output_dict

