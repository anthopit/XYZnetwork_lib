{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:35:38.130200Z",
     "end_time": "2023-04-14T19:35:38.488921Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessing as pp\n",
    "from classes import transportnetwork as tn\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69638/69638 [00:05<00:00, 12365.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "G = pp.create_network_from_trailway(\"../../data/Railway Data_JL.xlsx\")\n",
    "TN = tn.TransportNetwork(G, pos_argument=['lon', 'lat'], time_arguments=['dep_time', 'arr_time'], distance_argument='distance')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T09:40:08.242588Z",
     "end_time": "2023-04-13T09:40:24.547725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\r\n",
      "Requirement already satisfied: torch-scatter in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (2.1.1+pt20cu117)\r\n",
      "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\r\n",
      "Requirement already satisfied: torch-sparse in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (0.6.17+pt20cu117)\r\n",
      "Requirement already satisfied: scipy in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-sparse) (1.10.1)\r\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from scipy->torch-sparse) (1.24.2)\r\n",
      "Requirement already satisfied: torch-geometric in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (2.4.0)\r\n",
      "Requirement already satisfied: numpy in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (1.24.2)\r\n",
      "Requirement already satisfied: requests in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (2.28.2)\r\n",
      "Requirement already satisfied: tqdm in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (4.65.0)\r\n",
      "Requirement already satisfied: scipy in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (1.10.1)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (5.9.4)\r\n",
      "Requirement already satisfied: scikit-learn in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (1.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\r\n",
      "Requirement already satisfied: pyparsing in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from requests->torch-geometric) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from requests->torch-geometric) (2022.12.7)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
    "!pip install torch-geometric"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T11:37:03.736738Z",
     "end_time": "2023-04-14T11:37:15.170597Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, ChebConv, GATConv, SAGEConv, GINConv, GraphConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import *"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [],
   "source": [
    "use_cuda_if_available = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T14:33:06.732923Z",
     "end_time": "2023-04-13T14:33:06.752156Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Dataset object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(edge_index=[2, 64155], lon=[2719], lat=[2719], dep_time=[64155], arr_time=[64155], train=[64155], train_max_speed=[64155], day=[64155], distance=[64155], num_nodes=2719)"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = from_networkx(TN.multidigraph)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T14:33:10.141633Z",
     "end_time": "2023-04-13T14:33:11.358893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "# Create a one-hot encoding of the node features\n",
    "data.x = torch.eye(data.num_nodes)[data.x]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T14:33:12.010971Z",
     "end_time": "2023-04-13T14:33:12.032404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_max_deg(data):\n",
    "    \"\"\"\n",
    "    Find the max degree across all nodes in graphs.\n",
    "    \"\"\"\n",
    "    max_deg = 0\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    num_nodes = data.num_nodes\n",
    "    deg = degree(row, num_nodes)\n",
    "    deg = max(deg).item()\n",
    "    if deg > max_deg:\n",
    "        max_deg = int(deg)\n",
    "    return max_deg\n",
    "\n",
    "def cat_one_hot_feature(data, max_degree, in_degree=False, cat=True, features=['degree']):\n",
    "\n",
    "    if 'degree' in features:\n",
    "        idx, x = data.edge_index[1 if in_degree else 0], data.x\n",
    "        deg = degree(idx, data.num_nodes, dtype=torch.long)\n",
    "        deg = F.one_hot(deg, num_classes=max_degree + 1).to(torch.float)\n",
    "\n",
    "        if x is not None and cat:\n",
    "            x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "            data.x = torch.cat([x, deg.to(x.dtype)], dim=-1)\n",
    "        else:\n",
    "            data.x = deg\n",
    "\n",
    "    return data\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "\n",
    "class TransportNetworkDataset(InMemoryDataset):\n",
    "    def __init__(self, root, input_file, transform=None, pre_transform=None):\n",
    "        self.input_file = input_file\n",
    "        self.transform = transform\n",
    "        super(TransportNetworkDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # If you have any raw data files, you can list them here\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # This is the name of the processed file that will be saved to disk\n",
    "        return ['transport_network_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Downloading is not needed for this example\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "\n",
    "        G = pp.create_network_from_trailway(self.input_file)\n",
    "\n",
    "        # Add features to network\n",
    "        # Add node degree\n",
    "\n",
    "        # Create the Data object\n",
    "        data = from_networkx(G)\n",
    "\n",
    "        # Create a one-hot encoding with one_hot function\n",
    "        # Get a tensor with all the nodes ids\n",
    "        indices = torch.tensor([i for i in range(data.num_nodes)])\n",
    "        one_hot_encoding = one_hot(indices, num_classes=data.num_nodes)\n",
    "        data.x = one_hot_encoding\n",
    "\n",
    "        # Add node features\n",
    "        # Add node degree\n",
    "        max_degree = get_max_deg(data)\n",
    "        data = cat_one_hot_feature(data, max_degree)\n",
    "\n",
    "        # Split the data\n",
    "        train_ratio = 0.2\n",
    "        num_nodes = data.x.shape[0]\n",
    "        num_train = int(num_nodes * train_ratio)\n",
    "        idx = [i for i in range(num_nodes)]\n",
    "\n",
    "        np.random.shuffle(idx)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[idx[:num_train]] = True\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask[idx[num_train:]] = True\n",
    "\n",
    "        data.train_mask = train_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        # create a tensor with only 1 class\n",
    "        data.y = torch.ones(data.num_nodes, dtype=torch.long)\n",
    "\n",
    "        data = Data(x=data.x, edge_index=data.edge_index, train_mask=data.train_mask, test_mask=data.test_mask, y=data.y)\n",
    "\n",
    "        print(data.edge_index.shape)\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_list = [self.transform(data) for data in data_list]\n",
    "\n",
    "        # Store the processed data\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T23:24:29.338218Z",
     "end_time": "2023-04-13T23:24:29.402983Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69638/69638 [00:15<00:00, 4509.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64155])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "# Define augmentations\n",
    "transform = T.Compose([\n",
    "    T.RandomNodeSplit(num_splits=2),\n",
    "])\n",
    "\n",
    "dataset = TransportNetworkDataset(root='./transport_dataset', input_file=\"../../data/Railway Data_JL.xlsx\", transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T23:24:31.437856Z",
     "end_time": "2023-04-13T23:25:11.050192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "DATA_SPLIT = [0.7, 0.2, 0.1] # Train / val / test split ratio\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_data_percent=1.0):\n",
    "    \"\"\"\n",
    "    Splits the data into train / val / test sets.\n",
    "    Args:\n",
    "        dataset (list): all graphs in the dataset.\n",
    "        train_data_percent (float): Fraction of training data\n",
    "            which is labelled. (default 1.0)\n",
    "    \"\"\"\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    n = len(dataset)\n",
    "    train_split, val_split, test_split = DATA_SPLIT\n",
    "\n",
    "    train_end = int(n * DATA_SPLIT[0] * train_data_percent)\n",
    "    val_end = train_end + int(n * DATA_SPLIT[1])\n",
    "    train_dataset, val_dataset, test_dataset = [i for i in dataset[:train_end]], [i for i in dataset[train_end:val_end]], [i for i in dataset[val_end:]]\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:16:54.944141Z",
     "end_time": "2023-04-13T20:16:54.959101Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [
    "# Data loader\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:16:55.454144Z",
     "end_time": "2023-04-13T20:16:55.475310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = contrastive_loss(out)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def contrastive_loss(out):\n",
    "    batch_size = out.size(0) // 2\n",
    "    emb_size = out.size(1)\n",
    "    out1, out2 = out.split(batch_size, dim=0)\n",
    "    labels = torch.arange(batch_size, device=out.device)\n",
    "    mask = torch.eye(batch_size * 2, device=out.device)\n",
    "    sim_matrix = torch.mm(out, out.t()) / emb_size\n",
    "    sim_matrix = sim_matrix - mask * 1e9\n",
    "    pos_mask = labels.unsqueeze(1) == labels.unsqueeze(0)\n",
    "    neg_mask = ~pos_mask\n",
    "    pos_sim = sim_matrix[pos_mask].view(batch_size, -1)\n",
    "    neg_sim = sim_matrix[neg_mask].view(batch_size, -1)\n",
    "    logits = torch.cat([pos_sim, neg_sim], dim=1)\n",
    "    labels = torch.zeros(batch_size, dtype=torch.long, device=out.device)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:16:55.887864Z",
     "end_time": "2023-04-13T20:16:55.935031Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        self.hidden_channels = hidden_channels\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, self.hidden_channels)\n",
    "        self.conv2 = GCNConv(self.hidden_channels, data.num_nodes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:16:56.361097Z",
     "end_time": "2023-04-13T20:16:56.434743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(hidden_channels=16).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:16:56.848137Z",
     "end_time": "2023-04-13T20:16:56.904327Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GCN.forward() missing 1 required positional argument: 'edge_index'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[327], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[326], line 7\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      6\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m----> 7\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m loss \u001B[38;5;241m=\u001B[39m contrastive_loss(out)\n\u001B[1;32m      9\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: GCN.forward() missing 1 required positional argument: 'edge_index'"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    loss = train()\n",
    "    print(f\"Epoch {epoch}, Loss {loss:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T14:44:43.024740Z",
     "end_time": "2023-04-13T14:44:43.156776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
      "Extracting data/MUTAG/MUTAG.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Load the dataset\n",
    "dataset = TUDataset(root='data', name='MUTAG')\n",
    "train_dataset = dataset[:150]\n",
    "val_dataset = dataset[150:]\n",
    "\n",
    "# Define the graph encoder\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Define the contrastive loss function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        batch_size = z1.size(0)\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        sim = torch.matmul(z, z.t())\n",
    "        sim /= self.temperature\n",
    "        mask = torch.eye(batch_size * 2, device=z.device).bool()\n",
    "        loss = F.cross_entropy(sim[mask], torch.arange(batch_size * 2, device=z.device))\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:36:40.411535Z",
     "end_time": "2023-04-13T20:36:40.957531Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "transform = RandomNodeSplit(num_splits=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:39:46.801843Z",
     "end_time": "2023-04-13T20:39:46.859041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        data_aug1 = transform(data)\n",
    "        data_aug2 = transform(data)\n",
    "        z1 = model(data.x, data.edge_index)\n",
    "        z2 = model(data_aug1.x, data_aug1.edge_index)\n",
    "        z3 = model(data_aug2.x, data_aug2.edge_index)\n",
    "        loss = loss_fn(z1, z2) + loss_fn(z1, z3)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.num_graphs\n",
    "        return train_loss / len(loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:39:47.940111Z",
     "end_time": "2023-04-13T20:39:47.973798Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "outputs": [],
   "source": [
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            data_aug1 = transform(data)\n",
    "            data_aug2 = transform(data)\n",
    "            z1 = model(data.x, data.edge_index)\n",
    "            z2 = model(data_aug1.x, data_aug1.edge_index)\n",
    "            z3 = model(data_aug2.x, data_aug2.edge_index)\n",
    "            loss = loss_fn(z1, z2) + loss_fn(z1, z3)\n",
    "            val_loss += loss.item() * data.num_graphs\n",
    "            return val_loss / len(loader.dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:39:49.861690Z",
     "end_time": "2023-04-13T20:39:49.887461Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 0.01\n",
    "hidden_channels = 32\n",
    "out_channels = 16\n",
    "temperature = 0.5\n",
    "num_epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:39:50.943039Z",
     "end_time": "2023-04-13T20:39:50.946635Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [],
   "source": [
    "model = GraphEncoder(dataset.num_node_features, hidden_channels, out_channels).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = ContrastiveLoss(temperature=temperature)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:39:52.335887Z",
     "end_time": "2023-04-13T20:39:52.341777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 38 but got size 36 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[348], line 9\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m batched_data\n\u001B[1;32m      7\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(data)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[348], line 4\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(data_list)\u001B[0m\n\u001B[1;32m      2\u001B[0m batched_data \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m data_list[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys:\n\u001B[0;32m----> 4\u001B[0m     batched_data[key] \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata_list\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m batched_data\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Sizes of tensors must match except in dimension 0. Expected size 38 but got size 36 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def collate_fn(data_list):\n",
    "    batched_data = {}\n",
    "    for key in data_list[0].keys:\n",
    "        batched_data[key] = torch.cat([data[key] for data in data_list], dim=0)\n",
    "    return batched_data\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for data in train_loader:\n",
    "    print(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-13T20:47:51.921327Z",
     "end_time": "2023-04-13T20:47:51.928656Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[346], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(model, DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m), loss_fn, device)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m02d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Val Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[331], line 4\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, loss_fn, device)\u001B[0m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m      3\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[1;32m      5\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      6\u001B[0m     data_aug1 \u001B[38;5;241m=\u001B[39m transform(data)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    204\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:150\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    147\u001B[0m             \u001B[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001B[39;00m\n\u001B[1;32m    148\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]\n\u001B[0;32m--> 150\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem_type))\n",
      "\u001B[0;31mTypeError\u001B[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = evaluate(model, DataLoader(val_dataset, batch_size=32), loss_fn, device)\n",
    "    print(f'Epoch: {epoch:02d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_max_deg(data):\n",
    "    \"\"\"\n",
    "    Find the max degree across all nodes in graphs.\n",
    "    \"\"\"\n",
    "    max_deg = 0\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    num_nodes = data.num_nodes\n",
    "    deg = degree(row, num_nodes)\n",
    "    deg = max(deg).item()\n",
    "    if deg > max_deg:\n",
    "        max_deg = int(deg)\n",
    "    return max_deg\n",
    "\n",
    "def cat_one_hot_feature(data, max_degree, in_degree=False, cat=True, features=['degree']):\n",
    "\n",
    "    if 'degree' in features:\n",
    "        idx, x = data.edge_index[1 if in_degree else 0], data.x\n",
    "        deg = degree(idx, data.num_nodes, dtype=torch.long)\n",
    "        deg = F.one_hot(deg, num_classes=max_degree + 1).to(torch.float)\n",
    "\n",
    "        if x is not None and cat:\n",
    "            x = x.view(-1, 1) if x.dim() == 1 else x\n",
    "            data.x = torch.cat([x, deg.to(x.dtype)], dim=-1)\n",
    "        else:\n",
    "            data.x = deg\n",
    "\n",
    "    return data\n",
    "\n",
    "from torch_geometric.data import Data, InMemoryDataset, Dataset\n",
    "\n",
    "class TransportNetworkDataset(InMemoryDataset):\n",
    "    def __init__(self, root, input_file, transform=None, pre_transform=None):\n",
    "        self.input_file = input_file\n",
    "        self.transform = transform\n",
    "        super(TransportNetworkDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # If you have any raw data files, you can list them here\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # This is the name of the processed file that will be saved to disk\n",
    "        return ['transport_network_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # Downloading is not needed for this example\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "\n",
    "        G = pp.create_network_from_trailway(self.input_file)\n",
    "\n",
    "        # Add features to network\n",
    "        # Add node degree\n",
    "\n",
    "        # Create the Data object\n",
    "        data = from_networkx(G)\n",
    "\n",
    "        # Create a one-hot encoding with one_hot function\n",
    "        # Get a tensor with all the nodes ids\n",
    "        indices = torch.tensor([i for i in range(data.num_nodes)])\n",
    "        one_hot_encoding = one_hot(indices, num_classes=data.num_nodes)\n",
    "        data.x = one_hot_encoding\n",
    "\n",
    "        # Add node features\n",
    "        # Add node degree\n",
    "        max_degree = get_max_deg(data)\n",
    "        data = cat_one_hot_feature(data, max_degree)\n",
    "\n",
    "        # Split the data\n",
    "        DATA_SPLIT = [0.7, 0.2, 0.1]\n",
    "        num_nodes = data.x.shape[0]\n",
    "        num_train = int(num_nodes * DATA_SPLIT[0])\n",
    "        num_val = int(num_nodes * DATA_SPLIT[1])\n",
    "        num_test = int(num_nodes * DATA_SPLIT[2])\n",
    "        idx = [i for i in range(num_nodes)]\n",
    "\n",
    "        np.random.shuffle(idx)\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        train_mask[idx[:num_train]] = True\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask[idx[num_train:num_train + num_val]] = True\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask[idx[num_train + num_val:]] = True\n",
    "\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        # create a tensor with only 1 class\n",
    "        data.y = torch.ones(data.num_nodes, dtype=torch.long)\n",
    "\n",
    "        data = Data(x=data.x, edge_index=data.edge_index, train_mask=data.train_mask, test_mask=data.test_mask, y=data.y)\n",
    "\n",
    "        print(data.edge_index.shape)\n",
    "\n",
    "        data_list.append(data)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data_list = [self.transform(data) for data in data_list]\n",
    "\n",
    "        # Store the processed data\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\t\"\"\"\n",
    "\tDataset class that returns a graph and its augmented view in get() call.\n",
    "\tAugmentations are applied sequentially based on the augment_list.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, dataset):\n",
    "\t\tsuper(MyDataset, self).__init__()\n",
    "\n",
    "\t\tself.dataset = dataset\n",
    "\n",
    "\tdef get_positive_sample(self, current_graph):\n",
    "\n",
    "         node_mask_prob = 0.2\n",
    "         edge_mask_prob = 0.1\n",
    "         edge_perturb_prob = 0.1\n",
    "\n",
    "         graph_temp = current_graph\n",
    "\n",
    "         if node_mask_prob > 0:\n",
    "            num_nodes = graph_temp.x.size(0)\n",
    "            mask = torch.rand(num_nodes) < node_mask_prob\n",
    "            mask = mask.view(-1, 1).to(torch.float)\n",
    "            graph_temp.x = graph_temp.x * mask\n",
    "\n",
    "         if edge_mask_prob > 0 or edge_perturb_prob > 0:\n",
    "            # Convert edge_index to a COO format\n",
    "            graph_temp.edge_index, _ = add_self_loops(graph_temp.edge_index, num_nodes=graph_temp.num_nodes)\n",
    "\n",
    "            # Dropout edges\n",
    "            if edge_mask_prob > 0:\n",
    "                graph_temp.edge_index, _ = dropout_adj(graph_temp.edge_index, p=edge_mask_prob)\n",
    "\n",
    "            # Perturb edges\n",
    "            if edge_perturb_prob > 0:\n",
    "                edge_mask = torch.ones(graph_temp.edge_index.size(1)).to(torch.bool)\n",
    "                edge_mask = F.dropout(edge_mask, p=edge_perturb_prob, training=True)\n",
    "                mask = edge_mask.view(1, -1).repeat(2, 1)\n",
    "                perturb = torch.randn(graph_temp.edge_index.size(1), 2) * edge_mask.float().unsqueeze(-1)\n",
    "                graph_temp.edge_index = (graph_temp.edge_index + perturb).long()\n",
    "\n",
    "         return graph_temp\n",
    "\n",
    "\tdef get(self, idx):\n",
    "\t\tgraph_anchor = self.dataset[idx]\n",
    "\t\tgraph_pos = self.get_positive_sample(graph_anchor)\n",
    "\t\treturn PairData(graph_anchor.edge_index, graph_anchor.x, graph_pos.edge_index, graph_pos.x)\n",
    "\n",
    "\tdef len(self):\n",
    "\t\treturn len(self.dataset)\n",
    "\n",
    "\n",
    "class PairData(Data):\n",
    "\t\"\"\"\n",
    "\tUtility function to return a pair of graphs in dataloader.\n",
    "\tAdapted from https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, edge_index_anchor = None, x_anchor = None, edge_index_pos = None, x_pos = None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.edge_index_anchor = edge_index_anchor\n",
    "\t\tself.x_anchor = x_anchor\n",
    "\n",
    "\t\tself.edge_index_pos = edge_index_pos\n",
    "\t\tself.x_pos = x_pos\n",
    "\n",
    "\tdef __inc__(self, key, value, *args, **kwargs):\n",
    "\t\tif key == \"edge_index_anchor\":\n",
    "\t\t\treturn self.x_anchor.size(0)\n",
    "\t\tif key == \"edge_index_pos\":\n",
    "\t\t\treturn self.x_pos.size(0)\n",
    "\t\telse:\n",
    "\t\t\treturn super().__inc__(key, value, *args, **kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:33:52.092856Z",
     "end_time": "2023-04-14T12:33:52.285970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69638/69638 [00:13<00:00, 5168.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64155])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = TransportNetworkDataset(root='./transport_dataset', input_file=\"../../data/Railway Data_JL.xlsx\", transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:33:54.046704Z",
     "end_time": "2023-04-14T12:34:29.684903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [],
   "source": [
    "loader = DataLoader(MyDataset(dataset), num_workers=2, batch_size=64,\n",
    "\t\t\t\t\t\tshuffle=True)\n",
    "\n",
    "# loader = DataLoader(MyDataset(dataset), num_workers=2, batch_size=64,\n",
    "# \t\t\t\t\t\tshuffle=True, follow_batch=[\"x_anchor\", \"x_pos\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T11:49:52.591646Z",
     "end_time": "2023-04-14T11:49:52.607956Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, feat_dim, hidden_dim, n_layers):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.acts = nn.ModuleList()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        a = nn.ReLU()\n",
    "        for i in range(n_layers):\n",
    "            start_dim = hidden_dim if i else feat_dim\n",
    "            conv = GCNConv(start_dim, hidden_dim)\n",
    "            self.convs.append(conv)\n",
    "            self.acts.append(a)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.acts[i](x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:07:15.697280Z",
     "end_time": "2023-04-14T12:07:15.703570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(128, 64)\n",
      "    (1-2): 2 x GCNConv(64, 64)\n",
      "  )\n",
      "  (acts): ModuleList(\n",
      "    (0-2): 3 x ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GCN(feat_dim=128, hidden_dim=64, n_layers=3)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:07:24.830350Z",
     "end_time": "2023-04-14T12:07:24.917369Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "def infonce(readout_anchor, readout_positive, tau=0.5, norm=True):\n",
    "    \"\"\"\n",
    "    The InfoNCE (NT-XENT) loss in contrastive learning. The implementation\n",
    "    follows the paper `A Simple Framework for Contrastive Learning of\n",
    "    Visual Representations <https://arxiv.org/abs/2002.05709>`.\n",
    "    Args:\n",
    "        readout_anchor, readout_positive: Tensor of shape [batch_size, feat_dim]\n",
    "        tau: Float. Usually in (0,1].\n",
    "        norm: Boolean. Whether to apply normlization.\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = readout_anchor.shape[0]\n",
    "    sim_matrix = torch.einsum(\"ik,jk->ij\", readout_anchor, readout_positive)\n",
    "\n",
    "    if norm:\n",
    "        readout_anchor_abs = readout_anchor.norm(dim=1)\n",
    "        readout_positive_abs = readout_positive.norm(dim=1)\n",
    "        sim_matrix = sim_matrix / torch.einsum(\"i,j->ij\", readout_anchor_abs, readout_positive_abs)\n",
    "\n",
    "    sim_matrix = torch.exp(sim_matrix / tau)\n",
    "    pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
    "    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
    "    loss = - torch.log(loss).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run(epoch, mode, dataloader):\n",
    "\tif mode == \"train\":\n",
    "\t\tmodel.train()\n",
    "\telif mode == \"val\" or mode == \"test\":\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\tlosses = []\n",
    "\tfor data in dataloader:\n",
    "\t\tdata.to(device)\n",
    "\n",
    "\t\t# readout_anchor is the embedding of the original datapoint x on passing through the model\n",
    "\t\treadout_anchor = model((data.x_anchor,\n",
    "\t\t\t\t\t\t\t\tdata.edge_index_anchor, data.x_anchor_batch))\n",
    "\n",
    "\t\t# readout_positive is the embedding of the positively augmented x on passing through the model\n",
    "\t\treadout_positive = model((data.x_pos,\n",
    "\t\t\t\t\t\t\t\t\tdata.edge_index_pos, data.x_pos_batch))\n",
    "\n",
    "\t\t# negative samples for calculating the contrastive loss is computed in contrastive_fn\n",
    "\t\tloss = infonce(readout_anchor, readout_positive)\n",
    "\n",
    "\t\tif mode == \"train\":\n",
    "\t\t\t# backprop\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t# keep track of loss values\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\t# gather the results for the epoch\n",
    "\tepoch_loss = sum(losses) / len(losses)\n",
    "\treturn epoch_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T12:25:25.790638Z",
     "end_time": "2023-04-14T12:25:25.994411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py\", line 258, in __getitem__\n    data = self.get(self.indices()[idx])\n  File \"/tmp/ipykernel_211312/691676025.py\", line 159, in get\n    graph_pos = self.get_positive_sample(graph_anchor)\n  File \"/tmp/ipykernel_211312/691676025.py\", line 150, in get_positive_sample\n    edge_mask = F.dropout(edge_mask, p=edge_perturb_prob, training=True)\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/functional.py\", line 1252, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: result type Float can't be cast to the desired output type Bool\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[399], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m best_train_loss, best_val_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m), \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m20\u001B[39m):\n\u001B[0;32m----> 9\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m run(epoch, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m, loader)\n\u001B[1;32m     11\u001B[0m     log \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, Train Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m, Val Loss: \u001B[39m\u001B[38;5;132;01m{:.3f}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "Cell \u001B[0;32mIn[398], line 34\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(epoch, mode, dataloader)\u001B[0m\n\u001B[1;32m     31\u001B[0m \tmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     33\u001B[0m losses \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m dataloader:\n\u001B[1;32m     35\u001B[0m \tdata\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     37\u001B[0m \t\u001B[38;5;66;03m# readout_anchor is the embedding of the original datapoint x on passing through the model\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/_utils.py:644\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    640\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    641\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    642\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m--> 644\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py\", line 258, in __getitem__\n    data = self.get(self.indices()[idx])\n  File \"/tmp/ipykernel_211312/691676025.py\", line 159, in get\n    graph_pos = self.get_positive_sample(graph_anchor)\n  File \"/tmp/ipykernel_211312/691676025.py\", line 150, in get_positive_sample\n    edge_mask = F.dropout(edge_mask, p=edge_perturb_prob, training=True)\n  File \"/home/anthony/Desktop/Group Project/XYZnetwork_lib/venv/lib/python3.10/site-packages/torch/nn/functional.py\", line 1252, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: result type Float can't be cast to the desired output type Bool\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(os.path.join(\"logs\", \"ssl_model\")):\n",
    "    os.makedirs(os.path.join(\"logs\", \"ssl_model\"))\n",
    "\n",
    "best_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = run(epoch, \"train\", loader)\n",
    "    val_loss = run(epoch, \"val\", loader)\n",
    "    log = \"Epoch {}, Train Loss: {:.3f}, Val Loss: {:.3f}\"\n",
    "    print(log.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    # save model\n",
    "    is_best_loss = False\n",
    "    if val_loss < best_val_loss:\n",
    "        best_epoch, best_train_loss, best_val_loss, is_best_loss = \\\n",
    "                                            epoch, train_loss, val_loss, True\n",
    "\n",
    "    model.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch,\n",
    "                          best_train_loss, best_val_loss, is_best_loss)\n",
    "\n",
    "print(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
    "print(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network creation: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69638/69638 [00:15<00:00, 4632.59it/s]\n"
     ]
    }
   ],
   "source": [
    "G = pp.create_network_from_trailway(\"../../data/Railway Data_JL.xlsx\")\n",
    "TN = tn.TransportNetwork(G, pos_argument=['lon', 'lat'], time_arguments=['dep_time', 'arr_time'], distance_argument='distance')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:36:46.505308Z",
     "end_time": "2023-04-14T19:37:18.107533Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_max_deg(data):\n",
    "    \"\"\"\n",
    "    Find the max degree across all nodes in graphs.\n",
    "    \"\"\"\n",
    "    max_deg = 0\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    num_nodes = data.num_nodes\n",
    "    deg = degree(row, num_nodes)\n",
    "    deg = max(deg).item()\n",
    "    if deg > max_deg:\n",
    "        max_deg = int(deg)\n",
    "    return max_deg\n",
    "\n",
    "def cat_one_hot_feature(data, max_degree, in_degree=False, cat=True, features=['degree']):\n",
    "\n",
    "    feature_output = []\n",
    "    for feature in features:\n",
    "        if 'degree' in features:\n",
    "            idx, x = data.edge_index[1 if in_degree else 0], data.x\n",
    "            deg = degree(idx, data.num_nodes, dtype=torch.long)\n",
    "            deg = F.one_hot(deg, num_classes=max_degree + 1).to(torch.float)\n",
    "            feature_output.append(deg)\n",
    "\n",
    "    for f in feature_output:\n",
    "        if x is not None and cat:\n",
    "            data.x = torch.cat([data.x, f], dim=-1)\n",
    "        else:\n",
    "            data.x = f\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:37:36.750113Z",
     "end_time": "2023-04-14T19:37:36.763424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m max_degree \u001B[38;5;241m=\u001B[39m get_max_deg(data)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(max_degree)\n\u001B[0;32m----> 6\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[43mcat_one_hot_feature\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_degree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_degree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdegree\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m num_nodes \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mnum_nodes\n\u001B[1;32m      9\u001B[0m train_ratio \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.8\u001B[39m\n",
      "Cell \u001B[0;32mIn[3], line 21\u001B[0m, in \u001B[0;36mcat_one_hot_feature\u001B[0;34m(data, max_degree, in_degree, cat, features)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdegree\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n\u001B[1;32m     20\u001B[0m     idx, x \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39medge_index[\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m in_degree \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m], data\u001B[38;5;241m.\u001B[39mx\n\u001B[0;32m---> 21\u001B[0m     deg \u001B[38;5;241m=\u001B[39m degree(idx, data\u001B[38;5;241m.\u001B[39mnum_nodes, dtype\u001B[38;5;241m=\u001B[39m\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mlong)\n\u001B[1;32m     22\u001B[0m     deg \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mone_hot(deg, num_classes\u001B[38;5;241m=\u001B[39mmax_degree \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat)\n\u001B[1;32m     23\u001B[0m     feature_output\u001B[38;5;241m.\u001B[39mappend(deg)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import from_networkx, degree\n",
    "import torch\n",
    "\n",
    "data = from_networkx(TN.get_higher_complexity())\n",
    "max_degree = get_max_deg(data)\n",
    "print(max_degree)\n",
    "data = cat_one_hot_feature(data, max_degree, in_degree=False, cat=True, features=['degree'])\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2\n",
    "\n",
    "num_train_nodes = int(num_nodes * train_ratio)\n",
    "num_val_nodes = int(num_nodes * val_ratio)\n",
    "\n",
    "indices = np.arange(num_nodes)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:num_train_nodes]\n",
    "val_indices = indices[num_train_nodes:num_train_nodes + num_val_nodes]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:24.395277Z",
     "end_time": "2023-04-14T17:07:25.399219Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_geometric\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GCNConv\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential, Linear, ReLU\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGNN\u001B[39;00m(\u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28msuper\u001B[39m(GNN, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.fc = Sequential(Linear(128, 64), ReLU(), Linear(64, 32))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:25.398756Z",
     "end_time": "2023-04-14T17:07:25.411797Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment_data(data, node_mask_rate=0.2, edge_perturb_rate=0.2):\n",
    "    # Node attribute masking\n",
    "    node_mask = torch.rand(data.num_nodes) < node_mask_rate\n",
    "    data.x[node_mask] = 0\n",
    "\n",
    "    # Edge perturbation\n",
    "    edge_indices = torch.tensor(list(data.edge_index.T.cpu().numpy()))\n",
    "    edge_mask = torch.rand(len(edge_indices)) < edge_perturb_rate\n",
    "    edge_indices[edge_mask] = torch.randint(0, data.num_nodes, (edge_mask.sum(), 2)).to(edge_indices.device)\n",
    "    data.edge_index = edge_indices.T\n",
    "    return data\n",
    "\n",
    "class GraphCL_GNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphCL_GNN, self).__init__()\n",
    "        self.gnn = GNN()\n",
    "\n",
    "    def forward(self, data, data_aug):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x_aug, edge_index_aug = data_aug.x, data_aug.edge_index\n",
    "\n",
    "        return self.gnn(x, edge_index), self.gnn(x_aug, edge_index_aug)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:25.413001Z",
     "end_time": "2023-04-14T17:07:25.456037Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def info_nce_loss(z1, z2, temperature=0.1):\n",
    "    z1 = F.normalize(z1, p=2, dim=-1)\n",
    "    z2 = F.normalize(z2, p=2, dim=-1)\n",
    "    sim_matrix = torch.matmul(z1, z2.t())\n",
    "\n",
    "    pos_sim = torch.diag(sim_matrix)\n",
    "    neg_sim = torch.exp(sim_matrix) / (torch.exp(sim_matrix).sum(dim=-1, keepdim=True) - torch.exp(pos_sim).unsqueeze(-1))\n",
    "    neg_sim = neg_sim.sum(dim=-1)\n",
    "\n",
    "    loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + neg_sim)).mean()\n",
    "    return loss / temperature"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:25.659584Z",
     "end_time": "2023-04-14T17:07:25.720460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "model = GraphCL_GNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:07:26.129654Z",
     "end_time": "2023-04-14T17:07:26.493411Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Augment data\n",
    "    G_pyg_train = data.subgraph(train_mask)\n",
    "    G_pyg_train_aug = augment_data(G_pyg_train)\n",
    "\n",
    "    # Forward pass with the original and augmented data\n",
    "    z1_train, z2_train = model(G_pyg_train, G_pyg_train_aug.to(device))\n",
    "\n",
    "    # Compute InfoNCE loss for training set\n",
    "    train_loss = info_nce_loss(z1_train, z2_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "            # Augment data\n",
    "            G_pyg_val = data.subgraph(val_mask)\n",
    "            G_pyg_val_aug = augment_data(G_pyg_val)\n",
    "\n",
    "            # Forward pass with the original and augmented data\n",
    "            z1_val, z2_val = model(G_pyg_val, G_pyg_val_aug.to(device))\n",
    "\n",
    "            # Compute InfoNCE loss for validation set\n",
    "            val_loss = info_nce_loss(z1_val, z2_val)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, Train Loss: {train_loss.item()}, Val Loss: {val_loss.item()}')\n",
    "\n",
    "    # Save the best model weights based on validation loss\n",
    "    if val_loss.item() < best_loss:\n",
    "        best_loss = val_loss.item()\n",
    "        torch.save(model.state_dict(), \"best_model_weights.pth\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:33:36.083166Z",
     "end_time": "2023-04-14T17:36:13.586503Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z, _ = model(data, data)  # You can use the same graph twice as input, as the second graph is not used in the forward pass when not training"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:38:00.775868Z",
     "end_time": "2023-04-14T17:38:00.892420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from visualisation.visualisation import *\n",
    "\n",
    "plot_tsne_embedding(z)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T17:42:45.736040Z",
     "end_time": "2023-04-14T17:43:02.973548Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Set the number of clusters you want to obtain\n",
    "n_clusters = 4\n",
    "\n",
    "# Convert the embeddings tensor to a NumPy array\n",
    "embeddings = z\n",
    "\n",
    "# Run K-means clustering on the embeddings\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embeddings)\n",
    "\n",
    "# Get the cluster labels for each node\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "cluster_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:30:11.986235Z",
     "end_time": "2023-04-14T19:30:12.989872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-14T19:30:28.915098Z",
     "end_time": "2023-04-14T19:30:29.052380Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
